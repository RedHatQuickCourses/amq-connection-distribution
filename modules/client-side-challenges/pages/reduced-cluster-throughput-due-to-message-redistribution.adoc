#  Reduced cluster throughput due to message redistribution

This content is provided in Antora AsciiDoc format.

[#reduced-throughput-message-redistribution]
= Reduced Cluster Throughput Due to Message Redistribution

In an ActiveMQ Artemis cluster, ensuring optimal performance and message throughput is paramount. While the cluster architecture inherently provides mechanisms for high availability and fault tolerance, certain configurations and client behaviors can inadvertently lead to performance bottlenecks. One such significant challenge arises from the reliance on message redistribution, which can severely impact overall cluster throughput.

== Understanding Message Redistribution in ActiveMQ Artemis

ActiveMQ Artemis clusters are designed to be intelligent and resilient. A key feature is "Message Redistribution," which ensures that messages eventually reach their intended consumers even if they initially land on a broker without an attached consumer.

When a producer sends a message to a destination (e.g., a queue) in an ActiveMQ Artemis cluster, that message typically lands on one of the available broker nodes. Ideally, a consumer for that destination would be connected to the *same* broker node to process the message directly. However, if a consumer is exclusively connected to `Broker B`, but the message happens to land on `Broker A`, the cluster will automatically "bridge" or redistribute that message from `Broker A` to `Broker B`. This bridging mechanism ensures the message is eventually delivered to `Broker B` where the consumer can process it, thereby maintaining eventual consistency and message delivery guarantees.

== The Bottleneck: Why Redistribution Reduces Throughput

While message redistribution is a vital safety net for ensuring message delivery, *over-reliance* on it introduces significant overhead and can drastically reduce the overall throughput of the cluster.

1.  *Single Connection Bottleneck:* As highlighted in the context, without a proper connection pool or client-side load-balancing policy, applications frequently establish a single connection to a single broker within the cluster. This creates an immediate bottleneck, as all messages from that application are directed to one specific node, irrespective of where the consumers for those messages are located.

2.  *Inter-Broker Communication Overhead:* When messages need to be redistributed, they are not processed locally. Instead, they must be transmitted across the network from the initial broker (`Broker A`) to the broker where the consumer resides (`Broker B`). This inter-broker communication involves:
    *   Serialization and deserialization of messages.
    *   Network latency and bandwidth consumption.
    *   Processing cycles on both the source and destination brokers to manage the redistribution process.
    This "moving messages between nodes" introduces a measurable delay and consumes valuable broker resources that could otherwise be used for direct message processing. The context specifically notes that while the cluster can mitigate uneven distribution "by moving messages between nodes (configured via `redistribution-delay`), this process introduces significant overhead."

3.  *Reduced Overall Cluster Throughput:* The cumulative effect of this overhead is a reduction in the cluster's overall capacity to process messages. Instead of each broker contributing to the processing of new messages from producers, some brokers become burdened with the task of forwarding messages that should ideally have landed on a different node. This reliance on redistribution "reduces overall cluster throughput."

4.  *Paging Messages to Disk:* In high-volume scenarios, if a queue on a broker is receiving a large influx of messages that continuously need to be redistributed, the broker might struggle to keep up with the forwarding rate. When the in-memory message buffer fills up, ActiveMQ Artemis may be forced to page these pending messages to disk. Disk I/O operations are significantly slower than in-memory operations, further exacerbating the performance bottleneck and leading to increased latency. The context explicitly warns that this can "force high-volume queues to page pending messages to disk."

In essence, while ActiveMQ Artemis clusters offer server-side load balancing and message redistribution capabilities, these features are best utilized as intelligent routing mechanisms and fail-safe options, rather than primary methods for client-side message distribution. Optimal performance is achieved when messages are initially directed to a broker that already has an attached consumer, minimizing the need for costly inter-broker redistribution.

=== Identifying and Mitigating Redistribution Issues

Understanding the impact of message redistribution is the first step towards optimizing your ActiveMQ Artemis cluster. Future sections and hands-on activities will delve into practical solutions for achieving more even message distribution, primarily through client-side strategies, to maximize throughput and leverage the cluster's full potential without incurring the overhead of excessive redistribution. This involves configuring client connection factories to distribute connections across multiple brokers, thereby ensuring that messages are sent directly to appropriate nodes from the outset.

The goal is to move from a scenario where "an application typically establishes a single connection to a single broker, creating a bottleneck" to one where client connections are intelligently managed to "evenly distribute messages across its members."