#  Securing external access with SSL/TLS and network policies

```asciidoc
=== Securing External Access with SSL/TLS and Network Policies

Securing external access to your Red Hat AMQ Broker instances deployed on OpenShift is paramount to protect sensitive message data and prevent unauthorized operations. This section delves into two critical mechanisms for achieving robust security for external connectivity: SSL/TLS for encrypting data in transit and Network Policies for finely controlling network access.

==== Securing External Access with SSL/TLS

While the AMQ Broker's internal acceptors might be configured with SSL/TLS, ensuring secure communication *from external clients* into your OpenShift cluster and then to the broker is a separate, crucial step. OpenShift provides powerful features, particularly through Routes, to manage this.

===== Technical Explanation: SSL/TLS for External Access

SSL/TLS (Secure Sockets Layer/Transport Layer Security) is the standard cryptographic protocol for establishing secure communication channels over a computer network. When applied to external access for AMQ Broker, it ensures:

*   **Confidentiality:** Messages exchanged between the client and broker are encrypted, preventing eavesdropping.
*   **Integrity:** Data remains unaltered during transit.
*   **Authentication:** Clients can verify the identity of the broker (and optionally, the broker can verify the client).

On OpenShift, when exposing AMQ Broker via Routes, TLS termination can occur at different points, influencing the overall security posture:

*   **`passthrough` Termination:**
    *   This is the most secure option for end-to-end encryption.
    *   The OpenShift Router forwards encrypted TLS traffic directly to the AMQ Broker service without decrypting it.
    *   The AMQ Broker's acceptor *must* be configured for TLS. The client establishes a direct TLS connection with the broker.
    *   This ensures that the traffic remains encrypted from the client's perspective all the way to the broker pod.
*   **`edge` Termination:**
    *   The OpenShift Router terminates the TLS connection, decrypts the traffic, and then forwards *unencrypted* traffic to the AMQ Broker service.
    *   The AMQ Broker's acceptor can be configured for plain text.
    *   While convenient, this means traffic within the OpenShift cluster (from router to broker pod) is unencrypted, which might not meet stringent security requirements.
*   **`reencrypt` Termination:**
    *   The OpenShift Router terminates the initial TLS connection (from client to router).
    *   It then establishes a *new* TLS connection to the AMQ Broker service, re-encrypting the traffic.
    *   The AMQ Broker's acceptor *must* be configured for TLS.
    *   This provides end-to-end encryption but involves an additional TLS handshake and decryption/re-encryption overhead at the router.

For AMQ Broker, especially when dealing with sensitive data, **`passthrough` termination is generally recommended** if the broker's acceptor is already configured for TLS. This leverages the broker's own certificate management and provides the strongest end-to-end encryption model.

Custom certificates for the OpenShift Route (for `edge` or `reencrypt` termination) are managed as OpenShift `Secrets` and referenced by the Route. For `passthrough`, the broker's own certificate (configured in its acceptor) is used directly.

===== Hands-on Lab: Configuring an SSL/TLS Passthrough Route for AMQ Broker

In this lab, you will configure an OpenShift Route with `passthrough` TLS termination to securely expose an AMQ Broker instance that has a TLS-enabled acceptor.

*Prerequisites:*

*   An active OpenShift cluster where you have `developer` or `admin` permissions.
*   An AMQ Broker instance deployed via the AMQ Broker Operator.
*   The AMQ Broker instance must have at least one acceptor configured with SSL/TLS enabled. (This would typically be configured during the "Acceptor Configuration" objective). For example, an acceptor named `amqps-acceptor` on port `5671`.
*   The `oc` command-line tool configured for your cluster.

.Steps:
.  **Identify the AMQ Broker Service and TLS Port:**
    First, you need to find the `Service` created by the AMQ Broker Operator for your broker instance and determine the port of its TLS-enabled acceptor.
+
[source,shell]
----
oc get svc -l app.kubernetes.io/name=<broker-instance-name> -o yaml
----
+
Replace `<broker-instance-name>` with the actual name of your AMQ Broker instance (e.g., `my-broker`).
+
Look for the `ports` section in the output. You should see a port associated with your TLS acceptor (e.g., `targetPort: amqps`, `port: 5671`). Note the `name` (e.g., `amqps`) or `port` number that maps to your TLS acceptor.

.  **Create a `passthrough` TLS Route:**
    Now, create a `Route` manifest that targets the identified service and port, specifying `passthrough` termination.
+
[source,yaml]
----
# route-amqps-passthrough.yaml
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: <broker-instance-name>-amqps-passthrough
  labels:
    app: <broker-instance-name>
spec:
  to:
    kind: Service
    name: <broker-instance-name>-amqp-broker # Or whatever the service name is, often broker-instance-name-amqp-broker
    weight: 100
  port:
    targetPort: amqps # Use the name of your TLS-enabled port (e.g., 'amqps' or the specific port number like '5671')
  tls:
    termination: passthrough
  wildcardPolicy: None
----
+
*   Replace `<broker-instance-name>` with your broker's name.
*   Ensure `spec.to.name` matches the actual service name of your AMQ Broker.
*   Ensure `spec.port.targetPort` matches the `name` of the TLS port you identified in the previous step. If your TLS port doesn't have a named port, you can use the numerical `targetPort`.

.  **Apply the Route:**
    Deploy the route to your OpenShift project.
+
[source,shell]
----
oc apply -f route-amqps-passthrough.yaml
----

.  **Verify the Route:**
    Check that the route has been created and is active.
+
[source,shell]
----
oc get route <broker-instance-name>-amqps-passthrough
----
+
Note the `HOST/PORT` provided by the route. This is the external endpoint clients will use.

.  **Test Secure Connectivity (Optional, but Recommended):**
    To fully verify, you would typically use a messaging client (e.g., an AMQP 1.0 client) configured to connect to the route's hostname on the standard TLS port (e.g., `5671`) and enable SSL/TLS.
+
You can do a basic check using `openssl`:
+
[source,shell]
----
openssl s_client -connect <ROUTE_HOSTNAME>:443 -servername <ROUTE_HOSTNAME> -quiet
----
+
Replace `<ROUTE_HOSTNAME>` with the hostname obtained from `oc get route`. If the connection is successful, you should see certificate details and then the connection will hang, awaiting input, indicating a successful TLS handshake with the broker. Type `Q` and press Enter to exit.

This configuration establishes a secure, end-to-end encrypted channel for external clients to communicate with your AMQ Broker.

==== Restricting External Access with Network Policies

Even with SSL/TLS providing encryption, it's crucial to control *who* can establish a connection to your AMQ Broker. OpenShift's Network Policies provide a robust, Kubernetes-native way to define firewall-like rules for pods.

===== Technical Explanation: Network Policies

Kubernetes Network Policies are specifications that define how groups of pods are allowed to communicate with each other and with other network endpoints. They are crucial for implementing a zero-trust network model within your cluster.

*   **Policy Types:** Network Policies can define `Ingress` (incoming) rules, `Egress` (outgoing) rules, or both. For restricting external access *to* the broker, `Ingress` rules are key.
*   **`podSelector`:** Specifies which pods the Network Policy applies to. Traffic *to* these selected pods will be controlled by the policy.
*   **`ingress` Rules:** A list of rules defining allowed incoming connections. Each rule can specify:
    *   **`from`:** Who can send traffic to the selected pods. This can include:
        *   `ipBlock`: A CIDR range of IP addresses (e.g., `192.168.1.0/24`).
        *   `podSelector`: Pods within the *same* namespace that match the selector.
        *   `namespaceSelector`: Pods in *other* namespaces that match the selector.
    *   **`ports`:** Which ports are allowed for the incoming connections.
*   **Default Behavior:** If no `NetworkPolicy` selects a pod, all ingress and egress traffic is allowed to and from that pod. Once a `NetworkPolicy` *selects* a pod, all traffic not explicitly allowed by the policy is *denied*.

For external access to AMQ Broker, Network Policies work by applying rules to the AMQ Broker pods themselves. When an external client connects via an OpenShift Route or Service, the traffic eventually hits the broker pod. The Network Policy intercepts this and allows or denies it based on its rules, typically checking the source IP address or the originating pod/namespace (if the traffic is internal to OpenShift).

*Expert Insight:* When using `ipBlock` in `from` rules for external clients, be aware that the source IP seen by the Network Policy might be that of the OpenShift Router (for `edge`/`reencrypt` routes) or a NATed IP from the infrastructure, not necessarily the client's actual external IP. For `passthrough` routes, the source IP *should* be the client's. For precise external IP filtering, you might need to combine Network Policies with external firewall rules or load balancer features.

===== Hands-on Lab: Implementing Network Policies for AMQ Broker External Access

This lab guides you through creating a Network Policy to restrict external access to your AMQ Broker instance to specific IP ranges.

*Prerequisites:*

*   An active OpenShift cluster where you have `developer` or `admin` permissions.
*   An AMQ Broker instance deployed and externally accessible (e.g., via the `passthrough` route configured in the previous lab).
*   The `oc` command-line tool configured for your cluster.

.Steps:
.  **Identify the Labels of Your AMQ Broker Pods:**
    Network Policies use `podSelector` to determine which pods they apply to. The AMQ Broker Operator typically assigns labels that you can use.
+
[source,shell]
----
oc get pods -l app.kubernetes.io/name=<broker-instance-name> -o custom-columns=NAME:.metadata.name,LABELS:.metadata.labels
----
+
Look for labels like `app.kubernetes.io/component: broker`, `app.kubernetes.io/instance: <broker-instance-name>`, `app.kubernetes.io/name: broker`.
+
For example, if your broker instance is named `my-broker`, you might see labels like:
`app.kubernetes.io/name=broker,app.kubernetes.io/instance=my-broker`
+
You'll use these labels in your Network Policy's `podSelector`.

.  **Define a Network Policy to Restrict Ingress:**
    Create a `NetworkPolicy` manifest that restricts ingress to your AMQ Broker pods. In this example, we'll allow traffic only from a specific IP CIDR block and a specific port.
+
[source,yaml]
----
# netpol-broker-external-access.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-external-amq-broker-access
  namespace: <your-project-name> # Ensure this matches your OpenShift project
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: broker
      app.kubernetes.io/instance: <broker-instance-name> # Use the specific instance label
  policyTypes:
  - Ingress
  ingress:
  - from:
    - ipBlock:
        cidr: 192.168.1.0/24 # Replace with your allowed client IP CIDR
        # except:
        #   - 192.168.1.5/32 # Optionally exclude specific IPs within the block
    ports:
    - protocol: TCP
      port: 5671 # The port of your AMQPS acceptor
    # You can add more 'from' entries for other allowed sources
    # For example, to allow access from another namespace's pods:
    # - namespaceSelector:
    #     matchLabels:
    #       name: my-client-namespace
    #   podSelector:
    #     matchLabels:
    #       app: my-amqp-client
----
+
*   Replace `<your-project-name>` with the name of your OpenShift project.
*   Update `podSelector.matchLabels` with the specific labels that identify *your* AMQ Broker pods.
*   **CRITICALLY:** Replace `192.168.1.0/24` with the actual CIDR range of your allowed client machines or network segment.
*   Ensure `port: 5671` matches the port of your externally exposed AMQ Broker acceptor (e.g., the `targetPort` of your passthrough route).

.  **Apply the Network Policy:**
    Deploy the Network Policy to your OpenShift project.
+
[source,shell]
----
oc apply -f netpol-broker-external-access.yaml
----

.  **Verify the Network Policy:**
    Check that the Network Policy has been created.
+
[source,shell]
----
oc get networkpolicy allow-external-amq-broker-access
----

.  **Test Restricted Access:**
    *   **Test from an *Allowed* IP (or a Pod in an allowed `namespaceSelector`):**
        Try to connect to your AMQ Broker from a machine whose IP falls within the `cidr` block you specified. The connection should succeed.
    *   **Test from a *Denied* IP:**
        Try to connect to your AMQ Broker from a machine whose IP is *not* in the allowed `cidr` block. The connection attempt should fail (e.g., timeout, connection refused), demonstrating the Network Policy's enforcement.
+
NOTE: Testing from a denied IP might require you to have access to a machine outside the allowed `ipBlock` and ensure that the router is not blocking it before it reaches the policy.

By combining SSL/TLS with carefully crafted Network Policies, you build a robust defense-in-depth strategy, ensuring that external access to your Red Hat AMQ Broker is both encrypted and strictly controlled.

```