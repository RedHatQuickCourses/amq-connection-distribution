#  Techniques to distribute messages evenly across cluster members

In a high-performance messaging environment, the ability to distribute messages evenly across all members of an ActiveMQ Artemis cluster is paramount. This section details the critical techniques required to achieve this, focusing on client-side strategies to maximize throughput and prevent bottlenecks.

== Understanding the Challenges of Uneven Message Distribution

When clients connect to a JMS broker cluster like ActiveMQ Artemis, the default connection mechanisms often do not guarantee an even distribution of connections or messages across the cluster's nodes. The provided context highlights this issue, stating: "When clients connect to a JMS broker cluster (such as ActiveMQ Artemis), standard connection mechanisms do not automatically guarantee an even distribution of connections across the cluster."

This common oversight leads to several significant performance challenges:

*   **Client-Side Bottlenecks:** Without explicit client-side configuration, an application typically "establishes a single connection to a single broker, creating a bottleneck." This means all messages from that producer application are routed through one broker, regardless of the cluster's overall capacity.
*   **Inefficient Server-Side Redistribution:** While ActiveMQ Artemis clusters *can* move messages between nodes (e.g., via `redistribution-delay`), relying on this mechanism for load balancing is inefficient. The context explicitly warns that "Reliance on message redistribution reduces overall cluster throughput" because this process "introduces significant overhead."
*   **Risk of Disk Paging:** When a single broker becomes overloaded due to uneven message distribution, high-volume queues on that node may be "force[d] to page pending messages to disk." This significantly degrades performance, increases latency, and can lead to a less responsive messaging system.

The fundamental takeaway is that for optimal performance, message distribution should ideally occur at the client level, preventing any single broker from becoming a chokepoint.

== Client-Side Techniques for Even Message Distribution

To overcome the challenges of uneven distribution and achieve high throughput, client applications must implement strategies to distribute their connections and messages across all available brokers in an ActiveMQ Artemis cluster. The goal is to make the client "cluster-aware" rather than having it treat the cluster as a single endpoint.

The primary technique involves configuring the client's connection factory to list and connect to multiple brokers, often combined with a client-side load-balancing policy:

1.  **Multiple Broker Endpoints in Connection URI:**
    Client applications should be configured with a connection URI that includes the network addresses (host and port) of all desired ActiveMQ Artemis brokers in the cluster. For example, a typical client connection string might look like `tcp://broker1:61617,tcp://broker2:61617`. This tells the client library that there are multiple potential targets for connections.

2.  **Client-Side Load-Balancing Policy:**
    Once aware of multiple broker endpoints, the client library or the configured connection factory (e.g., JMS `ConnectionFactory`) should employ a load-balancing policy. Common policies include:
    *   **Round-Robin:** Connections are distributed sequentially to each broker in the list. The first connection goes to broker1, the second to broker2, the third back to broker1, and so on. This is a simple and effective way to evenly spread connection load.
    *   **Random:** Connections are established with a randomly selected broker from the list.
    *   **Sticky Session (with Failover):** While not ideal for even *initial* distribution, some policies allow a client to "stick" to one broker but automatically failover to another if the primary broker becomes unavailable. For active message distribution, a more dynamic approach is preferred.

    By implementing a client-side load-balancing policy, "an application typically establishes a single connection to a single broker" is replaced with a strategy where multiple connections are made and messages are directed to different brokers. This active distribution significantly reduces the need for server-side message redistribution and leverages the full capacity of the cluster.

3.  **Connection Pooling:**
    Utilizing a connection pool configured with the multiple broker endpoints and a load-balancing policy is another robust technique. A connection pool can manage a set of active connections to different brokers, distributing producer messages across these connections, thereby ensuring that the overall message traffic from the application is spread evenly. The context directly points to the necessity of a "connection pool or load-balancing policy" to avoid bottlenecks.

These client-side techniques ensure that message producers actively participate in distributing the load, thereby optimizing the cluster's performance, maintaining low latency, and preventing critical nodes from being overwhelmed. While the cluster itself offers "Server-Side Load Balancing" (where brokers route messages to nodes with consumers), client-side distribution is the proactive and most efficient method to ensure even message flow from the source.

== Hands-on Lab: Deploying the Cluster for Client Distribution

To effectively implement client-side message distribution, a fully functional ActiveMQ Artemis cluster with exposed services is a prerequisite. This hands-on activity focuses on deploying a two-node cluster using the Red Hat AMQ Operator on OpenShift, which forms the foundation for later client-side configuration.

The overarching "Solution" mentioned in the context is: "We will deploy a two node ActiveMQ Artemis cluster on OpenShift and try to evenly distribute messages across its members. To do so, install the Red Hat AMQ Operator and create an ActiveMQArtemis custom resource."

=== Step 1: Install the Red Hat AMQ Operator (Prerequisite)

Before deploying the cluster, ensure the Red Hat AMQ Operator is installed on your OpenShift cluster. This Operator simplifies the deployment and lifecycle management of ActiveMQ Artemis instances, including clustered configurations. (Refer to dedicated sections for detailed Operator installation steps).

=== Step 2: Create an ActiveMQArtemis Custom Resource (CR)

Once the AMQ Operator is active, define your ActiveMQ Artemis cluster by applying an `ActiveMQArtemis` Custom Resource (CR). This CR specifies the cluster's configuration, including how its services are exposed for client connections.

.Apply the following YAML to define and deploy a two-node ActiveMQ Artemis cluster:
[source,yaml]
----
apiVersion: broker.amq.io/v1beta1
kind: ActiveMQArtemis
metadata:
  name: broker
  namespace: broker # Ensure this namespace 'broker' exists or create it
spec:
  # This configuration will instruct the Operator to deploy a clustered ActiveMQ Artemis setup.
  # The specific number of nodes (e.g., two) is typically controlled by properties like 'size'
  # or implied by the clustering capabilities of the Operator for this kind.
  acceptors:
    - bindToAllInterfaces: true
      connectionsAllowed: -1
      expose: true
      name: broker
      port: 61617
      sslEnabled: true
      sslSecret: tls # Ensure a Kubernetes Secret named 'tls' exists for SSL
  addressSettings:
    addressSetting:
      - match: '#'
        # Further address-specific settings can be added here if needed.
        # The '#' match applies these settings globally to all addresses.
----

.Explanation of the `ActiveMQArtemis` Custom Resource:
*   `apiVersion: broker.amq.io/v1beta1` and `kind: ActiveMQArtemis`: These identify the resource type managed by the AMQ Operator.
*   `metadata.name: broker`: This is the logical name for your ActiveMQ Artemis instance within OpenShift.
*   `namespace: broker`: Specifies the OpenShift project (namespace) where the broker cluster will be deployed.
*   `spec.acceptors`: This crucial section defines how clients can connect to the broker instances.
    *   `bindToAllInterfaces: true`: Ensures the acceptor listens on all available network interfaces within the pod.
    *   `connectionsAllowed: -1`: Configures the acceptor to allow an unlimited number of client connections.
    *   `expose: true`: This instruction tells the AMQ Operator to create necessary OpenShift `Service` and `Route` (if Routes are enabled and available) resources. These expose the broker's listener ports to external client applications, making them discoverable.
    *   `name: broker`: A logical name for this specific client acceptor.
    *   `port: 61617`: The TCP/IP port on which clients will connect to the broker.
    *   `sslEnabled: true` and `sslSecret: tls`: Configures the acceptor to enforce SSL/TLS for client connections, referencing an OpenShift `Secret` named `tls` that must contain the necessary certificate and key.
*   `spec.addressSettings.addressSetting`: Defines default settings for messaging addresses (queues and topics). The `match: '#'` applies these settings globally to all addresses handled by the cluster.

By applying this `ActiveMQArtemis` CR, the AMQ Operator will provision and configure a two-node ActiveMQ Artemis cluster in your OpenShift environment. The `expose: true` setting is key, as it provides the discoverable endpoints that client applications will use. In a subsequent hands-on lab, you will develop client applications that leverage these exposed services, configure their connection factories to connect to multiple brokers, and thus distribute messages evenly across the cluster members.